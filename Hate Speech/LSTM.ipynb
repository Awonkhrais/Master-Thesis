{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "####################\n",
    "df = pd.read_csv('labeled_data.csv')\n",
    "df.head()\n",
    "####################\n",
    "\n",
    "# Lower case all the words of the tweet before any preprocessing\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "# Removing punctuations present in the text\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "\ttemp = str.maketrans('', '', punctuations_list)\n",
    "\treturn text.translate(temp)\n",
    "\n",
    "df['tweet']= df['tweet'].apply(lambda x: remove_punctuations(x))\n",
    "df.head()\n",
    "#########################################\n",
    "def remove_stopwords(text):\n",
    "\tstop_words = stopwords.words('english')\n",
    "\n",
    "\timp_words = []\n",
    "\n",
    "\t# Storing the important words\n",
    "\tfor word in str(text).split():\n",
    "\n",
    "\t\tif word not in stop_words:\n",
    "\n",
    "\t\t\t# Let's Lemmatize the word as well\n",
    "\t\t\t# before appending to the imp_words list.\n",
    "\n",
    "\t\t\tlemmatizer = WordNetLemmatizer()\n",
    "\t\t\tlemmatizer.lemmatize(word)\n",
    "\n",
    "\t\t\timp_words.append(word)\n",
    "\n",
    "\toutput = \" \".join(imp_words)\n",
    "\n",
    "\treturn output\n",
    "\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda text: remove_stopwords(text))\n",
    "df.head()\n",
    "\n",
    "##################################################\n",
    "\n",
    "def plot_word_cloud(data, typ):\n",
    "# Joining all the tweets to get the corpus\n",
    "    email_corpus = \" \".join(data['tweet'])\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "\n",
    "    # Forming the word cloud\n",
    "    wc = WordCloud(max_words = 100,\n",
    "                    width = 200,\n",
    "                    height = 100,\n",
    "                    collocations = False).generate(email_corpus)\n",
    "\n",
    "    # Plotting the wordcloud obtained above\n",
    "    plt.title(f'WordCloud for {typ} emails.', fontsize = 15)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wc)\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "plot_word_cloud(df[df['class']==2], typ='Neither')\n",
    "\n",
    "###########################################\n",
    "\n",
    "class_2 = df[df['class'] == 2]\n",
    "class_1 = df[df['class'] == 1].sample(n=3500)\n",
    "class_0 = df[df['class'] == 0]\n",
    "\n",
    "balanced_df = pd.concat([class_0, class_0, class_0, class_1, class_2], axis=0)\n",
    "\n",
    "plt.pie(balanced_df['class'].value_counts().values,\n",
    "\t\tlabels=balanced_df['class'].value_counts().index,\n",
    "\t\tautopct='%1.1f%%')\n",
    "plt.show()\n",
    "\n",
    "##############################################\n",
    "\n",
    "features = balanced_df['tweet']\n",
    "target = balanced_df['class']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttarget,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\trandom_state=22)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train)\n",
    "Y_test = pd.get_dummies(Y_test)\n",
    "Y_train.shape, Y_test.shape\n",
    "\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "token = Tokenizer(num_words=max_words,\n",
    "\t\t\t\tlower=True,\n",
    "\t\t\t\tsplit=' ')\n",
    "\n",
    "token.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "# training the tokenizer\n",
    "max_words = 5000\n",
    "token = Tokenizer(num_words=max_words,\n",
    "\t\t\t\tlower=True,\n",
    "\t\t\t\tsplit=' ')\n",
    "token.fit_on_texts(X_train)\n",
    "\n",
    "#Generating token embeddings\n",
    "Training_seq = token.texts_to_sequences(X_train)\n",
    "Training_pad = pad_sequences(Training_seq,\n",
    "\t\t\t\t\t\t\tmaxlen=max_len,  # Update here,\n",
    "\t\t\t\t\t\t\tpadding='post',\n",
    "\t\t\t\t\t\t\ttruncating='post')\n",
    "\n",
    "Testing_seq = token.texts_to_sequences(X_test)\n",
    "Testing_pad = pad_sequences(Testing_seq,\n",
    "\t\t\t\t\t\t\tmaxlen=max_len,\n",
    "\t\t\t\t\t\t\tpadding='post',\n",
    "\t\t\t\t\t\t\ttruncating='post')\n",
    "##############################\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "\tlayers.Embedding(max_words, 32, input_length=max_len),\n",
    "\tlayers.Bidirectional(layers.LSTM(16)),\n",
    "\tlayers.Dense(512, activation='relu', kernel_regularizer='l1'),\n",
    "\tlayers.BatchNormalization(),\n",
    "\tlayers.Dropout(0.3),\n",
    "\tlayers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\t\t\toptimizer='adam',\n",
    "\t\t\tmetrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "keras.utils.plot_model(\n",
    "\tmodel,\n",
    "\tshow_shapes=True,\n",
    "\tshow_dtype=True,\n",
    "\tshow_layer_activations=True\n",
    ")\n",
    "\n",
    "########################################\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "es = EarlyStopping(patience=3,\n",
    "\t\t\t\tmonitor = 'val_accuracy',\n",
    "\t\t\t\trestore_best_weights = True)\n",
    "\n",
    "lr = ReduceLROnPlateau(patience = 2,\n",
    "\t\t\t\t\tmonitor = 'val_loss',\n",
    "\t\t\t\t\tfactor = 0.5,\n",
    "\t\t\t\t\tverbose = 0)\n",
    "\n",
    "history = model.fit(Training_pad, Y_train,\n",
    "                    validation_data=(Testing_pad, Y_test),\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[lr, es])\n",
    "\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n",
    "plt.show()\n",
    "# Evaluate LSTM model accuracy\n",
    "loss, accuracy = model.evaluate(Testing_pad, Y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "y_pred = np.argmax(model.predict(Testing_pad), axis=-1)\n",
    "plot_confusion_matrix(confusion_matrix(np.argmax(Y_test, axis=-1), y_pred))\n",
    "###################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_pred = np.argmax(model.predict(Testing_pad), axis=-1)\n",
    "cm = confusion_matrix(np.argmax(Y_test, axis=-1), y_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else '.0f'  # Format as decimal fraction if normalize=True\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cm, classes=class_labels, normalize=True)  # Set normalize=True to display decimal fractions\n",
    "plt.show()\n",
    "print(classification_report(np.argmax(Y_test, axis=-1), y_pred))\n",
    "##############################\n",
    "# Traning Results\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions for training set\n",
    "y_train_pred = np.argmax(model.predict(Training_pad), axis=-1)\n",
    "\n",
    "# Calculate training set accuracy\n",
    "train_accuracy = accuracy_score(np.argmax(Y_train, axis=-1), y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "# Generate classification report for training set\n",
    "train_report = classification_report(np.argmax(Y_train, axis=-1), y_train_pred)\n",
    "print(\"Training Classification Report:\")\n",
    "print(train_report)\n",
    "###############################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_train_pred = np.argmax(model.predict(Training_pad), axis=-1)\n",
    "train_cm = confusion_matrix(np.argmax(Y_train, axis=-1), y_train_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(train_cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        train_cm = train_cm.astype('float') / train_cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(train_cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else '.0f'  # Format as decimal fraction if normalize=True\n",
    "    thresh = train_cm.max() / 2.\n",
    "    for i, j in itertools.product(range(train_cm.shape[0]), range(train_cm.shape[1])):\n",
    "        plt.text(j, i, format(train_cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if train_cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(train_cm, classes=class_labels, normalize=True)  # Set normalize=True to display decimal fractions\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
